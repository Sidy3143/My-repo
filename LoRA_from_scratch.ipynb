{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VwOtQt5uuwDJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class LoRALayer(nn.Module):\n",
        "  def __init__(self, in_features, out_features, rank=4, alpha=8):\n",
        "    super().__init__()\n",
        "\n",
        "    self.lora_B = nn.Parameter(torch.zeros(in_features, rank))\n",
        "    self.lora_A = nn.Parameter(torch.zeros(rank, out_features))\n",
        "    self.scale = alpha/rank\n",
        "\n",
        "    nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.scale * (x @ self.lora_B @ self.lora_A)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearWithLoRA(nn.Module):\n",
        "  def __init__(self, linear_layer, rank, alpha):\n",
        "    super().__init__()\n",
        "\n",
        "    self.linear_layer = linear_layer\n",
        "    self.lora = LoRALayer(linear_layer.in_features, linear_layer.out_features, rank, alpha)\n",
        "\n",
        "    self.lora_enabled = True\n",
        "\n",
        "    for param in self.linear_layer.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.lora_enabled:\n",
        "      return self.linear_layer(x) + self.lora(x)  # x @ (W + B@A)\n",
        "    else:\n",
        "      return self.linear_layer(x)"
      ],
      "metadata": {
        "id": "WyHrnRKmPxBw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply lora\n",
        "class ModelwithLoRA(nn.Module):\n",
        "  def __init__(self, base_model, target_modules, rank, alpha):\n",
        "    super().__init__()\n",
        "\n",
        "    self.base_model = base_model\n",
        "\n",
        "    for p in self.base_model.parameters():\n",
        "      p.requires_grad = False\n",
        "\n",
        "    self._apply_lora(self.base_model, target_modules, rank, alpha)\n",
        "\n",
        "  def _apply_lora(self, model, target_modules, rank, alpha):\n",
        "    for name, module in model.named_children():\n",
        "      if list(module.children()):\n",
        "        self._apply_lora(module, target_modules, rank, alpha)\n",
        "\n",
        "      if name in target_modules and isinstance(module, nn.Linear):\n",
        "        new_module = LinearWithLoRA(module, rank, alpha)\n",
        "        setattr(model, name, new_module) # model.name = new_module\n",
        "\n",
        "  def enabled_disable_lora(self, enabled):\n",
        "    for module in self.base_model.modules():\n",
        "      if isinstance(module, LinearWithLoRA):\n",
        "        module.lora_enabled = enabled\n",
        "\n",
        "  def forward(self, *args, enabled=True, **kwargs):\n",
        "    self.enabled_disable_lora(enabled)\n",
        "\n",
        "    return self.base_model(*args, **kwargs)"
      ],
      "metadata": {
        "id": "DzG9buuWgxWp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # Dummy image classification\n",
        "    self.layer1 = nn.Linear(784, 256)\n",
        "    self.layer2 = nn.Linear(256, 64)\n",
        "    self.layer3 = nn.Linear(64, 10)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.layer1(x))\n",
        "    x = self.relu(self.layer2(x))\n",
        "    x = self.layer3(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ZvtXJ45iuw1X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DummyModel()\n",
        "\n",
        "for name, module in model.named_modules():\n",
        "  print(f\"name: {name}, module: {module}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TPUMoOvHiB3",
        "outputId": "3d5f9cc8-959f-4485-81e4-1b94c2045be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name: , module: DummyModel(\n",
            "  (layer1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (layer2): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (layer3): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (relu): ReLU()\n",
            ")\n",
            "name: layer1, module: Linear(in_features=784, out_features=256, bias=True)\n",
            "name: layer2, module: Linear(in_features=256, out_features=64, bias=True)\n",
            "name: layer3, module: Linear(in_features=64, out_features=10, bias=True)\n",
            "name: relu, module: ReLU()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train it\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import transforms\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "8F1OcPDqUXxw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "train_data = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=10, shuffle=True)\n",
        "\n",
        "test_data = MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=10, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiFogByWW0Ts",
        "outputId": "4f7058ab-e1df-4e5e-c41e-ca65bf979230"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.4MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 495kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.93MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.54MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vlA_KdpV_Qu",
        "outputId": "15bb4c3e-aa29-4961-e67e-30f356061011"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = DummyModel()\n",
        "model.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "9KKM3iEeYnfz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trainning\n",
        "def train(model, dataloader):\n",
        "  params_to_optimize = [p for p in model.parameters() if p.requires_grad]\n",
        "  optimizer = torch.optim.Adam(params_to_optimize, lr=3e-4)\n",
        "\n",
        "  total_loss = 0\n",
        "  loop = tqdm(dataloader, total=len(dataloader), desc=\"trainning...\")\n",
        "  for n, (x, y) in enumerate(loop):\n",
        "    x = x.view(-1, 784).to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    output = model(x)\n",
        "    loss = loss_fn(output, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loop.set_postfix({'loss':loss.item()})\n",
        "    total_loss+= loss.item()\n"
      ],
      "metadata": {
        "id": "EHnsOeJUZIho"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "def test(model, testloader, enabled=True):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  wrong_counts = [0 for _ in range(10)]\n",
        "  for i, (x, y) in enumerate(tqdm(testloader, total=len(test_loader))):\n",
        "    x= x.view(-1, 28*28).to(device)\n",
        "    y  = y.to(device) #b\n",
        "\n",
        "    if isinstance(model, ModelwithLoRA):\n",
        "      output = model(x, enabled=enabled) #b, 10\n",
        "    else:\n",
        "      output = model(x) #b, 10\n",
        "\n",
        "    for i, pred in enumerate(output):\n",
        "      if torch.argmax(pred) == y[i]:\n",
        "        correct+= 1\n",
        "      else:\n",
        "        wrong_counts[y[i]] += 1\n",
        "      total += 1\n",
        "  acc = correct/total\n",
        "  print(f\"\\n Accuracy: {acc*100:.2f}%\")\n",
        "  print(\"--- wrong counts... ---\")\n",
        "  for i, count in enumerate(wrong_counts):\n",
        "    print(f\"wrong counts for number {i}: {count}\")"
      ],
      "metadata": {
        "id": "uReTqtG_Y0m9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0L7jUX5X3B2m",
        "outputId": "e771a23f-0652-4f06-cd14-28dc19b72c86"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "trainning...: 100%|██████████| 6000/6000 [00:35<00:00, 169.64it/s, loss=0.195]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6K6FbvY1U84",
        "outputId": "760fa89b-0878-441e-dfbe-64381a9810cc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:02<00:00, 344.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Accuracy: 96.69%\n",
            "--- wrong counts... ---\n",
            "wrong counts for number 0: 8\n",
            "wrong counts for number 1: 19\n",
            "wrong counts for number 2: 40\n",
            "wrong counts for number 3: 48\n",
            "wrong counts for number 4: 54\n",
            "wrong counts for number 5: 20\n",
            "wrong counts for number 6: 21\n",
            "wrong counts for number 7: 29\n",
            "wrong counts for number 8: 40\n",
            "wrong counts for number 9: 52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for name, param in self.named_parameters():\n",
        "#         is_lora_param = any(module_name in name for module_name in target_modules)\n",
        "#         if not is_lora_param:  # Freeze only non-LoRA parameters\n",
        "#             param.requires_grad = False"
      ],
      "metadata": {
        "id": "95nkbwkTJ04b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "base_model = copy.deepcopy(model)\n",
        "Lora_model = ModelwithLoRA(base_model, target_modules=['layer1', 'layer2', 'layer3'], rank=8, alpha=16)"
      ],
      "metadata": {
        "id": "v0B4Bgck1gQV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Lora_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3M8CAqw4X6X",
        "outputId": "1c3f7888-b3ca-4ff5-9187-1273c2c2b1b3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModelwithLoRA(\n",
              "  (base_model): DummyModel(\n",
              "    (layer1): LinearWithLoRA(\n",
              "      (linear_layer): Linear(in_features=784, out_features=256, bias=True)\n",
              "      (lora): LoRALayer()\n",
              "    )\n",
              "    (layer2): LinearWithLoRA(\n",
              "      (linear_layer): Linear(in_features=256, out_features=64, bias=True)\n",
              "      (lora): LoRALayer()\n",
              "    )\n",
              "    (layer3): LinearWithLoRA(\n",
              "      (linear_layer): Linear(in_features=64, out_features=10, bias=True)\n",
              "      (lora): LoRALayer()\n",
              "    )\n",
              "    (relu): ReLU()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Lora_model.base_model.layer3.lora.lora_B.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1XeS0IB7lHE",
        "outputId": "8f3f0483-f52c-409d-bbea-ef0eba6805df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exclude_indices = train_data.targets == 9\n",
        "train_data.data = train_data.data[exclude_indices]\n",
        "train_data.targets = train_data.targets[exclude_indices]"
      ],
      "metadata": {
        "id": "ClzaBgUA4w0b"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[500][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d5dOLhU8Kc2",
        "outputId": "9f369ffc-d7b1-407e-ab36-cd88cab7175d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = torch.utils.data.DataLoader(train_data, batch_size=10, shuffle=True)"
      ],
      "metadata": {
        "id": "SP7OiQDO8Cog"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(Lora_model, loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XONTOTzooT4",
        "outputId": "1c7bbf92-2635-46bf-fa18-0ac4f06a4fd2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "trainning...: 100%|██████████| 595/595 [00:03<00:00, 174.93it/s, loss=0.000493]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(Lora_model, loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYFnc8zGg3TK",
        "outputId": "63cbf935-921a-489e-e9c6-52d1becedc1f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|█████▉    | 595/1000 [00:02<00:01, 287.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Accuracy: 100.00%\n",
            "--- wrong counts... ---\n",
            "wrong counts for number 0: 0\n",
            "wrong counts for number 1: 0\n",
            "wrong counts for number 2: 0\n",
            "wrong counts for number 3: 0\n",
            "wrong counts for number 4: 0\n",
            "wrong counts for number 5: 0\n",
            "wrong counts for number 6: 0\n",
            "wrong counts for number 7: 0\n",
            "wrong counts for number 8: 0\n",
            "wrong counts for number 9: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(Lora_model, test_loader, enabled=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6bEN9Znq6hS",
        "outputId": "0497479e-d889-48bd-bf80-1efdd77ab1db"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:02<00:00, 342.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Accuracy: 96.69%\n",
            "--- wrong counts... ---\n",
            "wrong counts for number 0: 8\n",
            "wrong counts for number 1: 19\n",
            "wrong counts for number 2: 40\n",
            "wrong counts for number 3: 48\n",
            "wrong counts for number 4: 54\n",
            "wrong counts for number 5: 20\n",
            "wrong counts for number 6: 21\n",
            "wrong counts for number 7: 29\n",
            "wrong counts for number 8: 40\n",
            "wrong counts for number 9: 52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tot_param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "tot_param"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7CHdmPY4o_8",
        "outputId": "f4f0b2ba-993e-4c3d-8116-bf1e3ee74396"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "218058"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_param = sum(p.numel() for p in Lora_model.parameters() if p.requires_grad)\n",
        "lora_param"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hha9JDMSBi5_",
        "outputId": "f4731dc5-a18f-44a9-fef8-b4bf3c3f4467"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11472"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_param = sum(p.numel() for p in Lora_model.parameters() )\n",
        "total_param # 218058 + 11472"
      ],
      "metadata": {
        "id": "-HE3CC3FeKcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Try LoRA on GPT2 checkpoint I saved**"
      ],
      "metadata": {
        "id": "ZP_imoqgdBJP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pf5NgUbygq7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c49Wfo79dAsA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}